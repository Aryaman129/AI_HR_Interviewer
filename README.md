# ðŸ¤– AI HR Automation Platform

A production-grade, AI-powered recruitment automation platform with RAG-powered knowledge management, intelligent screening, and semantic search capabilities. Built with modern FastAPI architecture and PostgreSQL + pgvector for enterprise-scale deployments.

> **Latest Update (November 2025)**: Completed Week 3 - Company Knowledge API with RAG-powered semantic search using 768-dimensional JobBERT-v3 embeddings. Full production testing with 100% pass rate.

## ðŸŒŸ Features

### Core Capabilities
- **ðŸ” Intelligent Job Matching**: Semantic matching between candidates and job requirements using vector embeddings
- **ðŸ“„ Smart Resume Parsing**: Multi-format resume processing (PDF, DOCX, TXT) with automatic skill extraction
- **ðŸ¤– AI-Powered Screening**: Automated question generation, response evaluation, and candidate scoring
- **ðŸ§  RAG Knowledge Base**: Semantic search across company knowledge with real-time embedding generation
- **ðŸ“Š Advanced Analytics**: Comprehensive scoring, bias detection, and recommendation systems

### API Features (RESTful)
- **Jobs API**: Full CRUD with semantic search and candidate matching
- **Resumes API**: Upload, parse, and extract structured data from resumes
- **AI Screening API**: Generate questions, evaluate responses, track progress
- **Company Knowledge API**: RAG-powered document management with semantic search
- **OpenAPI Documentation**: Interactive API docs at `/docs` endpoint

### Technical Features
- **Production-Ready**: FastAPI with async/await, comprehensive error handling
- **Vector Database**: PostgreSQL + pgvector for semantic search (cosine similarity)
- **Real Embeddings**: JobBERT-v3 (768-dim) optimized for job/HR context
- **Scalable Architecture**: Service-oriented design with dependency injection
- **Type Safety**: Full Pydantic schema validation
- **100% Test Coverage**: E2E tests with real database and embeddings

## ðŸ—ï¸ Architecture

### Modern API-First Design

```
AI_HR_Platform/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ jobs.py           # Job management API
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ resumes.py        # Resume processing API
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ screening.py      # AI screening API
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ knowledge.py      # RAG knowledge base API
â”‚   â”‚   â”‚   â””â”€â”€ router.py             # API router configuration
â”‚   â”‚   â”œâ”€â”€ models/                   # SQLAlchemy ORM models
â”‚   â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â”‚   â”œâ”€â”€ candidate.py
â”‚   â”‚   â”‚   â”œâ”€â”€ resume.py
â”‚   â”‚   â”‚   â”œâ”€â”€ screening.py
â”‚   â”‚   â”‚   â””â”€â”€ company_knowledge.py
â”‚   â”‚   â”œâ”€â”€ schemas/                  # Pydantic schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â”‚   â”œâ”€â”€ resume.py
â”‚   â”‚   â”‚   â”œâ”€â”€ screening.py
â”‚   â”‚   â”‚   â””â”€â”€ company_knowledge.py
â”‚   â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_screening.py       # AI question generation
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py        # RAG + semantic search
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py  # Vector embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_provider.py       # LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ resume_parser.py      # Resume extraction
â”‚   â”‚   â”‚   â””â”€â”€ job_matcher.py        # Semantic matching
â”‚   â”‚   â””â”€â”€ db/
â”‚   â”‚       â”œâ”€â”€ database.py           # DB connection
â”‚   â”‚       â””â”€â”€ migrations/           # Alembic migrations
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ resumes/                  # Uploaded resume files
â”‚   â”‚   â””â”€â”€ uploads/                  # Temporary uploads
â”‚   â””â”€â”€ main.py                       # FastAPI application
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ app_config.json               # Application configuration
â”‚   â””â”€â”€ sample_jobs.json              # Sample data
â”œâ”€â”€ frontend/                         # Future: React/Vue frontend
â”œâ”€â”€ tests/                            # E2E and unit tests
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ alembic.ini                       # Database migration config
â””â”€â”€ README.md                         # This file
```

### Data Flow

```
Client Request
    â†“
FastAPI Endpoint (routes)
    â†“
Pydantic Validation (schemas)
    â†“
Service Layer (business logic)
    â†“
    â”œâ”€â†’ LLM Provider â†’ Ollama/OpenAI
    â”œâ”€â†’ Embedding Service â†’ JobBERT-v3 (768-dim)
    â”œâ”€â†’ RAG Service â†’ pgvector similarity search
    â””â”€â†’ Database (SQLAlchemy ORM)
    â†“
PostgreSQL + pgvector
```

## ðŸ› ï¸ Technology Stack

### Backend
- **Framework**: FastAPI 0.104+ (async/await, dependency injection)
- **Database**: PostgreSQL 15+ with pgvector extension
- **ORM**: SQLAlchemy 2.0+ with async support
- **Migration**: Alembic
- **Validation**: Pydantic v2

### AI/ML Models
- **LLM**: Ollama (Llama 3.1, Mistral) or OpenAI GPT-4
- **Embeddings**: JobBERT-v3 (768-dimensional, job-optimized)
- **Vector Search**: pgvector with cosine similarity
- **Resume Parsing**: spaCy + custom extractors

### Infrastructure
- **API Docs**: OpenAPI/Swagger (auto-generated)
- **Testing**: pytest with async support
- **Deployment**: Docker + docker-compose
- **CORS**: Configurable for frontend integration

### Future Integrations
- **Frontend**: React/Vue/Next.js (API-ready)
- **Workflow Automation**: n8n (webhook endpoints)
- **Email**: SendGrid/SMTP
- **Calendar**: Google Calendar, Outlook
- **Storage**: Minio/S3 for file storage

## ðŸ“‹ Prerequisites

### System Requirements
- **OS**: Windows 11, macOS, Linux
- **RAM**: 8 GB minimum, 16 GB recommended
- **Storage**: 5 GB free space
- **Python**: 3.10+
- **PostgreSQL**: 15+ with pgvector extension

### Required Software
- Python 3.10 or higher
- PostgreSQL with pgvector
- Ollama (for local LLM) or OpenAI API key
- Git

## ðŸš€ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/Aryaman129/AI_HR_Interviewer.git
cd AI_HR_Interviewer
```

### 2. Setup PostgreSQL with pgvector
```bash
# Install PostgreSQL 15+
# Then install pgvector extension
psql -U postgres
CREATE DATABASE ai_hr_db;
\c ai_hr_db
CREATE EXTENSION vector;
\q
```

### 3. Create Environment & Install Dependencies
```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # macOS/Linux

# Install requirements
pip install -r requirements.txt
```

### 4. Configure Environment
```bash
# Create .env file in root directory
cat > .env << EOF
DATABASE_URL=postgresql://postgres:password@localhost:5432/ai_hr_db
OLLAMA_BASE_URL=http://localhost:11434
LLM_PROVIDER=ollama
LLM_MODEL=llama3:latest
EMBEDDING_MODEL=TechWolfJobBert/jobbertv3
EOF
```

### 5. Run Database Migrations
```bash
cd backend
alembic upgrade head
```

### 6. Start Ollama (for local LLM)
```bash
# Download and install Ollama from https://ollama.ai
ollama pull llama3:latest
ollama serve
```

### 7. Launch API Server
```bash
cd backend
python main.py
# Or use uvicorn directly:
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 8. Access API Documentation
Open your browser to:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

## ðŸ“– API Usage Guide

### Creating a Job Posting
```bash
curl -X POST "http://localhost:8000/api/v1/jobs/" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Senior Python Developer",
    "company": "TechCorp",
    "description": "Looking for experienced Python developer...",
    "requirements": "5+ years Python. FastAPI experience required.",
    "location": "Remote",
    "salary_min": 100000,
    "salary_max": 150000,
    "employment_type": "full_time",
    "remote_option": true,
    "experience_level": "senior",
    "skills_required": ["Python", "FastAPI", "PostgreSQL", "Docker"],
    "department": "Engineering"
  }'
```

### Uploading and Parsing Resume
```bash
curl -X POST "http://localhost:8000/api/v1/resumes/upload" \
  -F "file=@resume.pdf" \
  -F "job_id=1"
```

### Starting AI Screening
```bash
curl -X POST "http://localhost:8000/api/v1/screening/start" \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": 1,
    "candidate_id": 123,
    "num_questions": 5,
    "question_types": ["technical", "behavioral", "role_specific"]
  }'
```

### Semantic Search in Knowledge Base
```bash
curl -X POST "http://localhost:8000/api/v1/knowledge/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are our company values regarding remote work?",
    "top_k": 5,
    "similarity_threshold": 0.2,
    "organization_id": 1
  }'
```

### Interactive API Documentation
Visit `http://localhost:8000/docs` for:
- Complete API reference
- Try-it-out functionality
- Request/response schemas
- Authentication details

## ðŸŽ¯ API Endpoints Overview

### Jobs API (`/api/v1/jobs`)
- `POST /` - Create job posting with auto-embedding
- `GET /` - List jobs with pagination and filtering
- `GET /{id}` - Get job details
- `PUT /{id}` - Update job posting
- `DELETE /{id}` - Delete job
- `POST /search` - Semantic job search
- `POST /{id}/match-candidates` - Find matching candidates

### Resumes API (`/api/v1/resumes`)
- `POST /upload` - Upload and parse resume
- `GET /` - List resumes with filtering
- `GET /{id}` - Get resume details
- `PUT /{id}` - Update resume data
- `DELETE /{id}` - Delete resume
- `POST /{id}/extract` - Re-extract skills and experience

### AI Screening API (`/api/v1/screening`)
- `POST /start` - Start screening session
- `GET /{id}` - Get screening details
- `POST /{id}/submit` - Submit answer to question
- `GET /{id}/evaluate` - Get evaluation results
- `POST /{id}/complete` - Finalize screening
- `GET /candidate/{id}` - Get candidate's screening history

### Company Knowledge API (`/api/v1/knowledge`)
- `POST /` - Create knowledge document
- `GET /` - List documents with pagination
- `GET /{id}` - Get document details
- `PUT /{id}` - Update document (auto-regenerate embedding)
- `DELETE /{id}` - Delete document
- `POST /search` - Semantic search with RAG
- `POST /bulk` - Bulk create documents
- `GET /stats/{org_id}` - Knowledge base statistics

## ðŸ”§ Configuration

### Environment Variables (`.env`)
```bash
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/ai_hr_db

# LLM Configuration
LLM_PROVIDER=ollama  # or 'openai'
LLM_MODEL=llama3:latest
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=your-key-here  # if using OpenAI

# Embedding Model
EMBEDDING_MODEL=TechWolfJobBert/jobbertv3
EMBEDDING_DIMENSION=768

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# CORS (for frontend)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# File Upload
MAX_UPLOAD_SIZE=10485760  # 10MB
UPLOAD_DIR=./data/uploads
```

### Application Config (`config/app_config.json`)
```json
{
  "models": {
    "llm": {
      "provider": "ollama",
      "model_name": "llama3:latest",
      "temperature": 0.7,
      "max_tokens": 2048
    },
    "embedding": {
      "model_name": "TechWolfJobBert/jobbertv3",
      "dimension": 768
    }
  },
  "screening": {
    "default_num_questions": 5,
    "passing_score": 65.0,
    "max_response_length": 2000
  },
  "rag": {
    "similarity_threshold": 0.2,
    "top_k": 5,
    "chunk_size": 1000
  }
}
```

## ðŸ“Š Testing

### Running Tests
```bash
cd backend
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=app --cov-report=html

# Run specific test file
pytest tests/test_production.py -v
```

### Test Results (Week 3)
```
âœ… 16/16 tests passing (100% pass rate)
- Job API tests: 4/4 âœ…
- Resume API tests: 4/4 âœ…
- Screening API tests: 4/4 âœ…
- Company Knowledge API tests: 4/4 âœ…

Test execution time: 2.30s
Real database: PostgreSQL
Real embeddings: JobBERT-v3 (768-dim)
```

### Production Testing
All APIs tested with:
- Real PostgreSQL database
- Real 768-dimensional embeddings
- End-to-end workflows
- Semantic search verification
- CRUD operations validation

## ï¿½ Deployment

### Docker Deployment
```bash
# Build and run with docker-compose
docker-compose up -d

# Services included:
# - FastAPI backend (port 8000)
# - PostgreSQL + pgvector (port 5432)
# - Ollama (port 11434)
```

### Production Considerations
- Use environment-specific `.env` files
- Enable HTTPS with reverse proxy (nginx, Caddy)
- Set up proper authentication (OAuth2, JWT)
- Configure rate limiting
- Enable monitoring (Prometheus, Grafana)
- Set up logging aggregation (ELK, Loki)
- Use managed PostgreSQL (AWS RDS, Azure Database)

## ðŸ”— Integration Capabilities

### n8n Workflow Automation ðŸŽ¯
Perfect for creating automated recruitment workflows:

**Example Workflows:**
1. **Auto-Resume Processing:**
   ```
   Email with Resume â†’ n8n â†’ Upload API â†’ Parse â†’ Match Jobs â†’ Notify HR
   ```

2. **Scheduled Screening:**
   ```
   New Application â†’ n8n â†’ Wait 24hrs â†’ Auto-Screen â†’ Email Results
   ```

3. **Multi-Platform Sourcing:**
   ```
   LinkedIn/Indeed â†’ n8n â†’ Extract â†’ Store â†’ Match â†’ Schedule Interview
   ```

**Setup:**
```bash
# 1. Install n8n
docker run -p 5678:5678 n8nio/n8n

# 2. Create webhook in our API
POST /api/v1/webhooks/n8n/{event}

# 3. Configure in n8n
Webhook URL: http://localhost:8000/api/v1/webhooks/n8n/resume_uploaded
```

### Other Integrations
- **Email**: SendGrid, SMTP, Mailgun
- **Calendar**: Google Calendar, Outlook, Cal.com
- **Storage**: AWS S3, Minio, Azure Blob
- **ATS**: Greenhouse, Lever (via webhooks)
- **CRM**: HubSpot, Salesforce
- **Slack/Teams**: Notifications and approvals

## ðŸ› ï¸ Troubleshooting

### Common Issues

**Database Connection Error**
```bash
# Check PostgreSQL is running
psql -U postgres -c "SELECT version();"

# Verify pgvector extension
psql -U postgres -d ai_hr_db -c "SELECT * FROM pg_extension WHERE extname='vector';"

# Update DATABASE_URL in .env
DATABASE_URL=postgresql://postgres:password@localhost:5432/ai_hr_db
```

**Ollama Connection Error**
```bash
# Start Ollama service
ollama serve

# Verify models are available
ollama list

# Pull required model
ollama pull llama3:latest
```

**Import/Dependency Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Install in development mode
pip install -e .
```

**Embedding Model Issues**
```bash
# The model downloads automatically on first use
# If issues persist, clear cache:
rm -rf ~/.cache/huggingface/
```

### Performance Optimization
- Use GPU for faster embedding generation
- Enable connection pooling in PostgreSQL
- Configure pgvector index for large datasets:
  ```sql
  CREATE INDEX ON company_knowledge USING ivfflat (embedding vector_cosine_ops);
  ```
- Cache frequently used embeddings
- Use async endpoints for concurrent requests

## ðŸ“ Database Schema

### Core Tables
```sql
-- Jobs
CREATE TABLE jobs (
    id SERIAL PRIMARY KEY,
    title VARCHAR(200),
    company_name VARCHAR(200),
    description TEXT,
    requirements JSONB,
    required_skills JSONB,
    embedding VECTOR(768),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Candidates & Resumes
CREATE TABLE candidates (
    id SERIAL PRIMARY KEY,
    name VARCHAR(200),
    email VARCHAR(200) UNIQUE,
    phone VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE resumes (
    id SERIAL PRIMARY KEY,
    candidate_id INTEGER REFERENCES candidates(id),
    file_path VARCHAR(500),
    extracted_text TEXT,
    skills JSONB,
    experience_years INTEGER,
    embedding VECTOR(768),
    created_at TIMESTAMP DEFAULT NOW()
);

-- AI Screening
CREATE TABLE screenings (
    id SERIAL PRIMARY KEY,
    job_id INTEGER REFERENCES jobs(id),
    candidate_id INTEGER REFERENCES candidates(id),
    questions JSONB,
    responses JSONB,
    evaluation JSONB,
    overall_score FLOAT,
    status VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Company Knowledge (RAG)
CREATE TABLE company_knowledge (
    id SERIAL PRIMARY KEY,
    organization_id INTEGER,
    doc_type VARCHAR(50),
    title VARCHAR(500),
    content TEXT,
    metadata JSONB,
    embedding VECTOR(768),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

## ðŸŽ¯ Roadmap

### âœ… Completed (Weeks 1-3)
- [x] Jobs API with semantic matching
- [x] Resume parsing and skill extraction
- [x] AI screening with question generation
- [x] RAG-powered knowledge base
- [x] Vector embeddings (768-dim JobBERT-v3)
- [x] Production testing (100% pass rate)

### ðŸš§ In Progress (Week 4)
- [ ] Interview Management API
- [ ] Real-time interview sessions
- [ ] Interview analytics

### ðŸ“… Planned (Weeks 5-8)
- [ ] Candidate pipeline management
- [ ] Email & notification system
- [ ] Calendar integration
- [ ] n8n workflow automation
- [ ] Video interview support
- [ ] Voice interview engine (Whisper + TTS)
- [ ] Multi-language support
- [ ] Frontend (React/Vue)
- [ ] ATS integrations
- [ ] Advanced analytics dashboard
## ï¿½ðŸ”® Future Enhancements

### Planned Features
- **Real Phone Integration**: Connect to actual phone systems
- **Advanced Analytics**: ML-powered insights and predictions
- **Multi-language Support**: Interviews in multiple languages
- **Video Interviews**: Computer vision for non-verbal analysis
- **ATS Integration**: Connect to existing HR systems

### Extensibility
- Plugin architecture for custom interview modules
- API endpoints for external integrations
- Custom scoring algorithms
- Industry-specific question sets

## ðŸ“„ License

MIT License - See LICENSE file for details.

## ðŸ¤ Contributing

Contributions welcome! Areas of focus:
- Frontend development (React/Vue)
- n8n workflow templates
- Additional AI screening question types
- Performance optimization
- Documentation improvements
- Test coverage expansion

## ðŸ“ž Support & Contact

- **Issues**: GitHub Issues
- **Documentation**: `/docs` endpoint
- **API Reference**: http://localhost:8000/docs

## ðŸ† Acknowledgments

- **JobBERT-v3**: TechWolf for job-optimized embeddings
- **pgvector**: PostgreSQL vector similarity search
- **FastAPI**: Modern Python web framework
- **Ollama**: Local LLM deployment made easy

---

**Built with â¤ï¸ for modern AI-powered recruitment automation**

*Last Updated: November 2025 - Week 3 Complete (Company Knowledge API with RAG)*
